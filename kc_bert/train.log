[2022-12-28 22:19:17,012] ============================
[2022-12-28 22:19:17,013] train_dataset                 :ratings_train.txt
[2022-12-28 22:19:17,013] dev_dataset                   :ratings_test.txt
[2022-12-28 22:19:17,013] gpt_model_hub_name            :bert-base-multilingual-cased
[2022-12-28 22:19:17,013] max_sequence_length           :512
[2022-12-28 22:19:17,013] epochs                        :3
[2022-12-28 22:19:17,013] lr                            :5e-05
[2022-12-28 22:19:17,013] train_batch_size              :32
[2022-12-28 22:19:17,013] test_batch_size               :32
[2022-12-28 22:19:17,013] output_dir                    :kc_bert/bert_2022.12.28_22.19.17
[2022-12-28 22:19:17,014] grad_clip                     :1.0
[2022-12-28 22:19:17,014] warmup_ratio                  :0.1
[2022-12-28 22:19:17,014] train_log_interval            :10
[2022-12-28 22:19:17,014] validation_interval           :200
[2022-12-28 22:19:17,014] save_interval                 :200
[2022-12-28 22:19:17,014] random_seed                   :0
[2022-12-28 22:19:17,014] tokenizer_name                :beomi/kcbert-base
[2022-12-28 22:19:17,014] model_name                    :beomi/kcbert-base
[2022-12-28 22:19:17,014] ============================
[2022-12-28 22:19:17,038] ============================
[2022-12-28 22:19:17,038] ============================
[2022-12-28 22:19:17,038] train_dataset                 :ratings_train.txt
[2022-12-28 22:19:17,038] train_dataset                 :ratings_train.txt
[2022-12-28 22:19:17,039] dev_dataset                   :ratings_test.txt
[2022-12-28 22:19:17,039] dev_dataset                   :ratings_test.txt
[2022-12-28 22:19:17,039] gpt_model_hub_name            :bert-base-multilingual-cased
[2022-12-28 22:19:17,039] max_sequence_length           :512
[2022-12-28 22:19:17,039] gpt_model_hub_name            :bert-base-multilingual-cased
[2022-12-28 22:19:17,039] epochs                        :3
[2022-12-28 22:19:17,039] max_sequence_length           :512
[2022-12-28 22:19:17,039] lr                            :5e-05
[2022-12-28 22:19:17,039] epochs                        :3
[2022-12-28 22:19:17,039] train_batch_size              :32
[2022-12-28 22:19:17,039] lr                            :5e-05
[2022-12-28 22:19:17,039] test_batch_size               :32
[2022-12-28 22:19:17,039] output_dir                    :kc_bert/bert_2022.12.28_22.19.17
[2022-12-28 22:19:17,039] train_batch_size              :32
[2022-12-28 22:19:17,039] grad_clip                     :1.0
[2022-12-28 22:19:17,039] test_batch_size               :32
[2022-12-28 22:19:17,039] warmup_ratio                  :0.1
[2022-12-28 22:19:17,039] output_dir                    :kc_bert/bert_2022.12.28_22.19.17
[2022-12-28 22:19:17,039] train_log_interval            :10
[2022-12-28 22:19:17,039] grad_clip                     :1.0
[2022-12-28 22:19:17,039] validation_interval           :200
[2022-12-28 22:19:17,040] warmup_ratio                  :0.1
[2022-12-28 22:19:17,040] save_interval                 :200
[2022-12-28 22:19:17,040] random_seed                   :0
[2022-12-28 22:19:17,040] train_log_interval            :10
[2022-12-28 22:19:17,040] tokenizer_name                :beomi/kcbert-base
[2022-12-28 22:19:17,040] validation_interval           :200
[2022-12-28 22:19:17,040] model_name                    :beomi/kcbert-base
[2022-12-28 22:19:17,040] save_interval                 :200
[2022-12-28 22:19:17,040] ============================
[2022-12-28 22:19:17,040] random_seed                   :0
[2022-12-28 22:19:17,040] tokenizer_name                :beomi/kcbert-base
[2022-12-28 22:19:17,040] model_name                    :beomi/kcbert-base
[2022-12-28 22:19:17,040] ============================
[2022-12-28 22:19:17,066] ============================
[2022-12-28 22:19:17,066] train_dataset                 :ratings_train.txt
[2022-12-28 22:19:17,066] dev_dataset                   :ratings_test.txt
[2022-12-28 22:19:17,067] gpt_model_hub_name            :bert-base-multilingual-cased
[2022-12-28 22:19:17,067] max_sequence_length           :512
[2022-12-28 22:19:17,067] epochs                        :3
[2022-12-28 22:19:17,067] lr                            :5e-05
[2022-12-28 22:19:17,067] train_batch_size              :32
[2022-12-28 22:19:17,067] test_batch_size               :32
[2022-12-28 22:19:17,067] output_dir                    :kc_bert/bert_2022.12.28_22.19.17
[2022-12-28 22:19:17,067] grad_clip                     :1.0
[2022-12-28 22:19:17,067] warmup_ratio                  :0.1
[2022-12-28 22:19:17,067] train_log_interval            :10
[2022-12-28 22:19:17,067] validation_interval           :200
[2022-12-28 22:19:17,067] save_interval                 :200
[2022-12-28 22:19:17,067] random_seed                   :0
[2022-12-28 22:19:17,068] tokenizer_name                :beomi/kcbert-base
[2022-12-28 22:19:17,068] model_name                    :beomi/kcbert-base
[2022-12-28 22:19:17,068] ============================
[2022-12-28 22:19:17,174] Added key: store_based_barrier_key:1 to store for rank: 2
[2022-12-28 22:19:17,174] Added key: store_based_barrier_key:1 to store for rank: 3
[2022-12-28 22:19:18,048] Added key: store_based_barrier_key:1 to store for rank: 1
[2022-12-28 22:19:18,050] Added key: store_based_barrier_key:1 to store for rank: 0
[2022-12-28 22:19:18,051] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[2022-12-28 22:19:18,058] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[2022-12-28 22:19:18,059] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[2022-12-28 22:19:18,059] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[2022-12-28 22:19:27,533] loading train dataset
[2022-12-28 22:19:27,575] loading train dataset
[2022-12-28 22:19:27,576] loading train dataset
[2022-12-28 22:19:27,577] loading train dataset
[2022-12-28 22:20:31,762] Reducer buckets have been rebuilt in this iteration.
[2022-12-28 22:20:31,762] Reducer buckets have been rebuilt in this iteration.
[2022-12-28 22:20:31,767] Reducer buckets have been rebuilt in this iteration.
[2022-12-28 22:20:31,768] Reducer buckets have been rebuilt in this iteration.
[2022-12-28 22:20:34,978] EP:0 global_step:10 loss:0.6881 perplexity:1.9899
[2022-12-28 22:20:37,853] EP:0 global_step:20 loss:0.7044 perplexity:2.0227
[2022-12-28 22:20:40,012] EP:0 global_step:30 loss:0.6338 perplexity:1.8848
[2022-12-28 22:20:42,075] EP:0 global_step:40 loss:0.5875 perplexity:1.7996
[2022-12-28 22:20:44,367] EP:0 global_step:50 loss:0.4875 perplexity:1.6283
[2022-12-28 22:20:46,568] EP:0 global_step:60 loss:0.4466 perplexity:1.5630
[2022-12-28 22:20:48,766] EP:0 global_step:70 loss:0.4572 perplexity:1.5796
[2022-12-28 22:20:50,866] EP:0 global_step:80 loss:0.4258 perplexity:1.5308
[2022-12-28 22:20:52,901] EP:0 global_step:90 loss:0.3217 perplexity:1.3794
[2022-12-28 22:20:54,999] EP:0 global_step:100 loss:0.3922 perplexity:1.4802
[2022-12-28 22:20:57,122] EP:0 global_step:110 loss:0.3983 perplexity:1.4893
[2022-12-28 22:20:59,256] EP:0 global_step:120 loss:0.3385 perplexity:1.4028
[2022-12-28 22:21:01,395] EP:0 global_step:130 loss:0.3542 perplexity:1.4250
[2022-12-28 22:21:03,547] EP:0 global_step:140 loss:0.3722 perplexity:1.4509
[2022-12-28 22:21:05,690] EP:0 global_step:150 loss:0.3359 perplexity:1.3992
[2022-12-28 22:21:07,846] EP:0 global_step:160 loss:0.2983 perplexity:1.3475
[2022-12-28 22:21:09,804] EP:0 global_step:170 loss:0.3528 perplexity:1.4230
[2022-12-28 22:21:11,917] EP:0 global_step:180 loss:0.3660 perplexity:1.4419
[2022-12-28 22:21:14,064] EP:0 global_step:190 loss:0.2959 perplexity:1.3443
[2022-12-28 22:21:16,116] EP:0 global_step:200 loss:0.3279 perplexity:1.3881
[2022-12-28 22:21:29,845] [EVAL] global_step:200 loss:0.3262 perplexity:1.3857
[2022-12-28 22:21:29,847] global_step: 200 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_200.pth
[2022-12-28 22:21:32,607] EP:0 global_step:210 loss:0.2566 perplexity:1.2925
[2022-12-28 22:21:34,640] EP:0 global_step:220 loss:0.3633 perplexity:1.4381
[2022-12-28 22:21:36,850] EP:0 global_step:230 loss:0.2520 perplexity:1.2866
[2022-12-28 22:21:39,147] EP:0 global_step:240 loss:0.3431 perplexity:1.4093
[2022-12-28 22:21:41,277] EP:0 global_step:250 loss:0.2897 perplexity:1.3360
[2022-12-28 22:21:43,352] EP:0 global_step:260 loss:0.2797 perplexity:1.3227
[2022-12-28 22:21:45,521] EP:0 global_step:270 loss:0.3436 perplexity:1.4100
[2022-12-28 22:21:47,804] EP:0 global_step:280 loss:0.3442 perplexity:1.4109
[2022-12-28 22:21:49,981] EP:0 global_step:290 loss:0.3123 perplexity:1.3666
[2022-12-28 22:21:52,118] EP:0 global_step:300 loss:0.2871 perplexity:1.3325
[2022-12-28 22:21:54,299] EP:0 global_step:310 loss:0.3067 perplexity:1.3589
[2022-12-28 22:21:56,337] EP:0 global_step:320 loss:0.3180 perplexity:1.3744
[2022-12-28 22:21:58,770] EP:0 global_step:330 loss:0.3383 perplexity:1.4026
[2022-12-28 22:22:00,901] EP:0 global_step:340 loss:0.2300 perplexity:1.2586
[2022-12-28 22:22:03,037] EP:0 global_step:350 loss:0.2609 perplexity:1.2981
[2022-12-28 22:22:05,258] EP:0 global_step:360 loss:0.2387 perplexity:1.2696
[2022-12-28 22:22:07,308] EP:0 global_step:370 loss:0.2507 perplexity:1.2849
[2022-12-28 22:22:09,444] EP:0 global_step:380 loss:0.2984 perplexity:1.3477
[2022-12-28 22:22:11,532] EP:0 global_step:390 loss:0.3321 perplexity:1.3938
[2022-12-28 22:22:13,587] EP:0 global_step:400 loss:0.3138 perplexity:1.3686
[2022-12-28 22:22:27,614] [EVAL] global_step:400 loss:0.2874 perplexity:1.3330
[2022-12-28 22:22:27,617] global_step: 400 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_400.pth
[2022-12-28 22:22:30,445] EP:0 global_step:410 loss:0.3164 perplexity:1.3721
[2022-12-28 22:22:32,581] EP:0 global_step:420 loss:0.2720 perplexity:1.3126
[2022-12-28 22:22:34,743] EP:0 global_step:430 loss:0.2923 perplexity:1.3396
[2022-12-28 22:22:37,284] EP:0 global_step:440 loss:0.2854 perplexity:1.3303
[2022-12-28 22:22:39,435] EP:0 global_step:450 loss:0.2762 perplexity:1.3181
[2022-12-28 22:22:41,551] EP:0 global_step:460 loss:0.2656 perplexity:1.3042
[2022-12-28 22:22:43,748] EP:0 global_step:470 loss:0.2709 perplexity:1.3111
[2022-12-28 22:22:45,868] EP:0 global_step:480 loss:0.2371 perplexity:1.2676
[2022-12-28 22:22:47,989] EP:0 global_step:490 loss:0.2563 perplexity:1.2922
[2022-12-28 22:22:50,104] EP:0 global_step:500 loss:0.2987 perplexity:1.3481
[2022-12-28 22:22:52,408] EP:0 global_step:510 loss:0.2928 perplexity:1.3402
[2022-12-28 22:22:54,543] EP:0 global_step:520 loss:0.2958 perplexity:1.3442
[2022-12-28 22:22:56,755] EP:0 global_step:530 loss:0.2817 perplexity:1.3254
[2022-12-28 22:22:58,959] EP:0 global_step:540 loss:0.2583 perplexity:1.2948
[2022-12-28 22:23:01,161] EP:0 global_step:550 loss:0.2415 perplexity:1.2732
[2022-12-28 22:23:03,284] EP:0 global_step:560 loss:0.2942 perplexity:1.3421
[2022-12-28 22:23:05,301] EP:0 global_step:570 loss:0.3253 perplexity:1.3845
[2022-12-28 22:23:07,483] EP:0 global_step:580 loss:0.2201 perplexity:1.2462
[2022-12-28 22:23:09,676] EP:0 global_step:590 loss:0.3088 perplexity:1.3618
[2022-12-28 22:23:11,808] EP:0 global_step:600 loss:0.2673 perplexity:1.3065
[2022-12-28 22:23:25,975] [EVAL] global_step:600 loss:0.2762 perplexity:1.3181
[2022-12-28 22:23:25,977] global_step: 600 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_600.pth
[2022-12-28 22:23:28,705] EP:0 global_step:610 loss:0.2596 perplexity:1.2965
[2022-12-28 22:23:30,988] EP:0 global_step:620 loss:0.2685 perplexity:1.3080
[2022-12-28 22:23:33,042] EP:0 global_step:630 loss:0.2661 perplexity:1.3049
[2022-12-28 22:23:35,094] EP:0 global_step:640 loss:0.3268 perplexity:1.3865
[2022-12-28 22:23:37,173] EP:0 global_step:650 loss:0.2697 perplexity:1.3096
[2022-12-28 22:23:39,329] EP:0 global_step:660 loss:0.3338 perplexity:1.3962
[2022-12-28 22:23:41,445] EP:0 global_step:670 loss:0.2448 perplexity:1.2774
[2022-12-28 22:23:43,599] EP:0 global_step:680 loss:0.2349 perplexity:1.2647
[2022-12-28 22:23:45,806] EP:0 global_step:690 loss:0.2917 perplexity:1.3388
[2022-12-28 22:23:48,004] EP:0 global_step:700 loss:0.3593 perplexity:1.4323
[2022-12-28 22:23:50,173] EP:0 global_step:710 loss:0.2444 perplexity:1.2769
[2022-12-28 22:23:52,342] EP:0 global_step:720 loss:0.2458 perplexity:1.2786
[2022-12-28 22:23:54,499] EP:0 global_step:730 loss:0.2821 perplexity:1.3259
[2022-12-28 22:23:56,635] EP:0 global_step:740 loss:0.2158 perplexity:1.2408
[2022-12-28 22:23:58,811] EP:0 global_step:750 loss:0.2847 perplexity:1.3294
[2022-12-28 22:24:01,113] EP:0 global_step:760 loss:0.2967 perplexity:1.3455
[2022-12-28 22:24:03,248] EP:0 global_step:770 loss:0.2687 perplexity:1.3083
[2022-12-28 22:24:05,498] EP:0 global_step:780 loss:0.2146 perplexity:1.2393
[2022-12-28 22:24:07,691] EP:0 global_step:790 loss:0.3387 perplexity:1.4031
[2022-12-28 22:24:09,791] EP:0 global_step:800 loss:0.3018 perplexity:1.3523
[2022-12-28 22:24:23,992] [EVAL] global_step:800 loss:0.2637 perplexity:1.3018
[2022-12-28 22:24:23,995] global_step: 800 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_800.pth
[2022-12-28 22:24:26,864] EP:0 global_step:810 loss:0.2851 perplexity:1.3299
[2022-12-28 22:24:28,992] EP:0 global_step:820 loss:0.2172 perplexity:1.2426
[2022-12-28 22:24:31,154] EP:0 global_step:830 loss:0.2253 perplexity:1.2527
[2022-12-28 22:24:33,321] EP:0 global_step:840 loss:0.2248 perplexity:1.2520
[2022-12-28 22:24:35,529] EP:0 global_step:850 loss:0.2410 perplexity:1.2725
[2022-12-28 22:24:37,756] EP:0 global_step:860 loss:0.2585 perplexity:1.2950
[2022-12-28 22:24:39,930] EP:0 global_step:870 loss:0.2730 perplexity:1.3138
[2022-12-28 22:24:42,036] EP:0 global_step:880 loss:0.2325 perplexity:1.2618
[2022-12-28 22:24:44,150] EP:0 global_step:890 loss:0.2568 perplexity:1.2928
[2022-12-28 22:24:46,414] EP:0 global_step:900 loss:0.2606 perplexity:1.2977
[2022-12-28 22:24:48,597] EP:0 global_step:910 loss:0.2263 perplexity:1.2539
[2022-12-28 22:24:50,663] EP:0 global_step:920 loss:0.2799 perplexity:1.3230
[2022-12-28 22:24:52,823] EP:0 global_step:930 loss:0.2617 perplexity:1.2992
[2022-12-28 22:24:55,040] EP:0 global_step:940 loss:0.2477 perplexity:1.2811
[2022-12-28 22:24:57,236] EP:0 global_step:950 loss:0.2753 perplexity:1.3169
[2022-12-28 22:24:59,342] EP:0 global_step:960 loss:0.2642 perplexity:1.3024
[2022-12-28 22:25:01,464] EP:0 global_step:970 loss:0.3572 perplexity:1.4294
[2022-12-28 22:25:03,603] EP:0 global_step:980 loss:0.2157 perplexity:1.2408
[2022-12-28 22:25:05,743] EP:0 global_step:990 loss:0.2408 perplexity:1.2723
[2022-12-28 22:25:07,899] EP:0 global_step:1000 loss:0.2678 perplexity:1.3070
[2022-12-28 22:25:22,126] [EVAL] global_step:1000 loss:0.2626 perplexity:1.3003
[2022-12-28 22:25:22,128] global_step: 1000 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_1000.pth
[2022-12-28 22:25:24,996] EP:0 global_step:1010 loss:0.2960 perplexity:1.3445
[2022-12-28 22:25:27,066] EP:0 global_step:1020 loss:0.1945 perplexity:1.2148
[2022-12-28 22:25:29,254] EP:0 global_step:1030 loss:0.2739 perplexity:1.3151
[2022-12-28 22:25:31,385] EP:0 global_step:1040 loss:0.2613 perplexity:1.2986
[2022-12-28 22:25:33,633] EP:0 global_step:1050 loss:0.2686 perplexity:1.3082
[2022-12-28 22:25:35,840] EP:0 global_step:1060 loss:0.2886 perplexity:1.3345
[2022-12-28 22:25:37,891] EP:0 global_step:1070 loss:0.2200 perplexity:1.2461
[2022-12-28 22:25:40,049] EP:0 global_step:1080 loss:0.2549 perplexity:1.2903
[2022-12-28 22:25:42,149] EP:0 global_step:1090 loss:0.1934 perplexity:1.2134
[2022-12-28 22:25:44,279] EP:0 global_step:1100 loss:0.2697 perplexity:1.3095
[2022-12-28 22:25:46,384] EP:0 global_step:1110 loss:0.2710 perplexity:1.3113
[2022-12-28 22:25:48,486] EP:0 global_step:1120 loss:0.2783 perplexity:1.3209
[2022-12-28 22:25:50,508] EP:0 global_step:1130 loss:0.2529 perplexity:1.2877
[2022-12-28 22:25:52,583] EP:0 global_step:1140 loss:0.2104 perplexity:1.2342
[2022-12-28 22:25:54,669] EP:0 global_step:1150 loss:0.3186 perplexity:1.3752
[2022-12-28 22:25:56,797] EP:0 global_step:1160 loss:0.2254 perplexity:1.2528
[2022-12-28 22:25:59,005] EP:0 global_step:1170 loss:0.2369 perplexity:1.2673
[2022-12-28 22:26:01,418] EP:1 global_step:1180 loss:0.2478 perplexity:1.2812
[2022-12-28 22:26:03,632] EP:1 global_step:1190 loss:0.2709 perplexity:1.3112
[2022-12-28 22:26:05,757] EP:1 global_step:1200 loss:0.1942 perplexity:1.2144
[2022-12-28 22:26:20,003] [EVAL] global_step:1200 loss:0.2463 perplexity:1.2793
[2022-12-28 22:26:20,005] global_step: 1200 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_1200.pth
[2022-12-28 22:26:22,830] EP:1 global_step:1210 loss:0.2458 perplexity:1.2786
[2022-12-28 22:26:24,950] EP:1 global_step:1220 loss:0.2605 perplexity:1.2976
[2022-12-28 22:26:27,244] EP:1 global_step:1230 loss:0.2222 perplexity:1.2489
[2022-12-28 22:26:29,477] EP:1 global_step:1240 loss:0.2485 perplexity:1.2821
[2022-12-28 22:26:31,645] EP:1 global_step:1250 loss:0.2435 perplexity:1.2756
[2022-12-28 22:26:33,686] EP:1 global_step:1260 loss:0.2012 perplexity:1.2228
[2022-12-28 22:26:35,825] EP:1 global_step:1270 loss:0.2441 perplexity:1.2765
[2022-12-28 22:26:37,967] EP:1 global_step:1280 loss:0.2508 perplexity:1.2851
[2022-12-28 22:26:40,143] EP:1 global_step:1290 loss:0.2064 perplexity:1.2292
[2022-12-28 22:26:42,317] EP:1 global_step:1300 loss:0.2738 perplexity:1.3150
[2022-12-28 22:26:44,559] EP:1 global_step:1310 loss:0.2074 perplexity:1.2305
[2022-12-28 22:26:46,666] EP:1 global_step:1320 loss:0.2147 perplexity:1.2395
[2022-12-28 22:26:48,823] EP:1 global_step:1330 loss:0.1496 perplexity:1.1613
[2022-12-28 22:26:50,892] EP:1 global_step:1340 loss:0.2160 perplexity:1.2411
[2022-12-28 22:26:52,997] EP:1 global_step:1350 loss:0.2441 perplexity:1.2765
[2022-12-28 22:26:55,161] EP:1 global_step:1360 loss:0.1799 perplexity:1.1971
[2022-12-28 22:26:57,283] EP:1 global_step:1370 loss:0.1708 perplexity:1.1862
[2022-12-28 22:26:59,380] EP:1 global_step:1380 loss:0.1676 perplexity:1.1824
[2022-12-28 22:27:01,476] EP:1 global_step:1390 loss:0.1957 perplexity:1.2161
[2022-12-28 22:27:03,640] EP:1 global_step:1400 loss:0.1735 perplexity:1.1894
[2022-12-28 22:27:17,920] [EVAL] global_step:1400 loss:0.2670 perplexity:1.3060
[2022-12-28 22:27:17,922] global_step: 1400 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_1400.pth
[2022-12-28 22:27:20,736] EP:1 global_step:1410 loss:0.1451 perplexity:1.1562
[2022-12-28 22:27:22,874] EP:1 global_step:1420 loss:0.1897 perplexity:1.2089
[2022-12-28 22:27:24,974] EP:1 global_step:1430 loss:0.1254 perplexity:1.1336
[2022-12-28 22:27:27,140] EP:1 global_step:1440 loss:0.1942 perplexity:1.2143
[2022-12-28 22:27:29,267] EP:1 global_step:1450 loss:0.2037 perplexity:1.2260
[2022-12-28 22:27:31,521] EP:1 global_step:1460 loss:0.1706 perplexity:1.1860
[2022-12-28 22:27:33,643] EP:1 global_step:1470 loss:0.1492 perplexity:1.1609
[2022-12-28 22:27:35,834] EP:1 global_step:1480 loss:0.1316 perplexity:1.1407
[2022-12-28 22:27:37,899] EP:1 global_step:1490 loss:0.1573 perplexity:1.1704
[2022-12-28 22:27:40,199] EP:1 global_step:1500 loss:0.1733 perplexity:1.1892
[2022-12-28 22:27:42,340] EP:1 global_step:1510 loss:0.1341 perplexity:1.1435
[2022-12-28 22:27:44,471] EP:1 global_step:1520 loss:0.1112 perplexity:1.1177
[2022-12-28 22:27:46,744] EP:1 global_step:1530 loss:0.1160 perplexity:1.1230
[2022-12-28 22:27:48,775] EP:1 global_step:1540 loss:0.0741 perplexity:1.0769
[2022-12-28 22:27:50,970] EP:1 global_step:1550 loss:0.1333 perplexity:1.1426
[2022-12-28 22:27:53,029] EP:1 global_step:1560 loss:0.1685 perplexity:1.1835
[2022-12-28 22:27:55,120] EP:1 global_step:1570 loss:0.1567 perplexity:1.1696
[2022-12-28 22:27:57,290] EP:1 global_step:1580 loss:0.2035 perplexity:1.2257
[2022-12-28 22:27:59,415] EP:1 global_step:1590 loss:0.1327 perplexity:1.1419
[2022-12-28 22:28:01,595] EP:1 global_step:1600 loss:0.1015 perplexity:1.1068
[2022-12-28 22:28:15,867] [EVAL] global_step:1600 loss:0.2885 perplexity:1.3345
[2022-12-28 22:28:15,870] global_step: 1600 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_1600.pth
[2022-12-28 22:28:18,699] EP:1 global_step:1610 loss:0.1121 perplexity:1.1186
[2022-12-28 22:28:20,941] EP:1 global_step:1620 loss:0.0896 perplexity:1.0938
[2022-12-28 22:28:23,032] EP:1 global_step:1630 loss:0.0981 perplexity:1.1031
[2022-12-28 22:28:25,238] EP:1 global_step:1640 loss:0.1298 perplexity:1.1386
[2022-12-28 22:28:27,381] EP:1 global_step:1650 loss:0.0962 perplexity:1.1009
[2022-12-28 22:28:29,494] EP:1 global_step:1660 loss:0.1184 perplexity:1.1257
[2022-12-28 22:28:31,616] EP:1 global_step:1670 loss:0.1392 perplexity:1.1494
[2022-12-28 22:28:33,792] EP:1 global_step:1680 loss:0.1350 perplexity:1.1445
[2022-12-28 22:28:35,985] EP:1 global_step:1690 loss:0.0940 perplexity:1.0986
[2022-12-28 22:28:38,165] EP:1 global_step:1700 loss:0.1343 perplexity:1.1437
[2022-12-28 22:28:40,415] EP:1 global_step:1710 loss:0.1204 perplexity:1.1279
[2022-12-28 22:28:42,628] EP:1 global_step:1720 loss:0.1070 perplexity:1.1130
[2022-12-28 22:28:44,751] EP:1 global_step:1730 loss:0.1704 perplexity:1.1857
[2022-12-28 22:28:46,785] EP:1 global_step:1740 loss:0.1216 perplexity:1.1293
[2022-12-28 22:28:48,939] EP:1 global_step:1750 loss:0.1234 perplexity:1.1313
[2022-12-28 22:28:51,110] EP:1 global_step:1760 loss:0.1544 perplexity:1.1670
[2022-12-28 22:28:53,254] EP:1 global_step:1770 loss:0.1192 perplexity:1.1266
[2022-12-28 22:28:55,399] EP:1 global_step:1780 loss:0.1463 perplexity:1.1575
[2022-12-28 22:28:57,611] EP:1 global_step:1790 loss:0.0951 perplexity:1.0998
[2022-12-28 22:28:59,759] EP:1 global_step:1800 loss:0.0665 perplexity:1.0687
[2022-12-28 22:29:14,056] [EVAL] global_step:1800 loss:0.3169 perplexity:1.3728
[2022-12-28 22:29:14,059] global_step: 1800 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_1800.pth
[2022-12-28 22:29:16,829] EP:1 global_step:1810 loss:0.1058 perplexity:1.1116
[2022-12-28 22:29:18,916] EP:1 global_step:1820 loss:0.1640 perplexity:1.1782
[2022-12-28 22:29:21,045] EP:1 global_step:1830 loss:0.1429 perplexity:1.1536
[2022-12-28 22:29:23,201] EP:1 global_step:1840 loss:0.1650 perplexity:1.1794
[2022-12-28 22:29:25,378] EP:1 global_step:1850 loss:0.1062 perplexity:1.1121
[2022-12-28 22:29:27,565] EP:1 global_step:1860 loss:0.1149 perplexity:1.1218
[2022-12-28 22:29:29,780] EP:1 global_step:1870 loss:0.1153 perplexity:1.1222
[2022-12-28 22:29:31,942] EP:1 global_step:1880 loss:0.1576 perplexity:1.1707
[2022-12-28 22:29:34,112] EP:1 global_step:1890 loss:0.1051 perplexity:1.1108
[2022-12-28 22:29:36,301] EP:1 global_step:1900 loss:0.1330 perplexity:1.1422
[2022-12-28 22:29:38,410] EP:1 global_step:1910 loss:0.0801 perplexity:1.0834
[2022-12-28 22:29:40,575] EP:1 global_step:1920 loss:0.1333 perplexity:1.1425
[2022-12-28 22:29:42,867] EP:1 global_step:1930 loss:0.1597 perplexity:1.1731
[2022-12-28 22:29:45,063] EP:1 global_step:1940 loss:0.1443 perplexity:1.1552
[2022-12-28 22:29:47,293] EP:1 global_step:1950 loss:0.1189 perplexity:1.1263
[2022-12-28 22:29:49,482] EP:1 global_step:1960 loss:0.1367 perplexity:1.1465
[2022-12-28 22:29:51,589] EP:1 global_step:1970 loss:0.1646 perplexity:1.1790
[2022-12-28 22:29:53,802] EP:1 global_step:1980 loss:0.1145 perplexity:1.1214
[2022-12-28 22:29:56,001] EP:1 global_step:1990 loss:0.0935 perplexity:1.0980
[2022-12-28 22:29:58,091] EP:1 global_step:2000 loss:0.1198 perplexity:1.1273
[2022-12-28 22:30:12,381] [EVAL] global_step:2000 loss:0.3267 perplexity:1.3864
[2022-12-28 22:30:12,384] global_step: 2000 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_2000.pth
[2022-12-28 22:30:15,281] EP:1 global_step:2010 loss:0.0962 perplexity:1.1010
[2022-12-28 22:30:17,481] EP:1 global_step:2020 loss:0.1025 perplexity:1.1080
[2022-12-28 22:30:19,702] EP:1 global_step:2030 loss:0.1044 perplexity:1.1101
[2022-12-28 22:30:21,885] EP:1 global_step:2040 loss:0.1719 perplexity:1.1876
[2022-12-28 22:30:23,993] EP:1 global_step:2050 loss:0.1381 perplexity:1.1481
[2022-12-28 22:30:26,113] EP:1 global_step:2060 loss:0.0960 perplexity:1.1007
[2022-12-28 22:30:28,383] EP:1 global_step:2070 loss:0.0993 perplexity:1.1044
[2022-12-28 22:30:30,541] EP:1 global_step:2080 loss:0.0759 perplexity:1.0789
[2022-12-28 22:30:32,643] EP:1 global_step:2090 loss:0.0736 perplexity:1.0764
[2022-12-28 22:30:34,849] EP:1 global_step:2100 loss:0.0914 perplexity:1.0957
[2022-12-28 22:30:37,000] EP:1 global_step:2110 loss:0.0671 perplexity:1.0694
[2022-12-28 22:30:39,195] EP:1 global_step:2120 loss:0.1504 perplexity:1.1623
[2022-12-28 22:30:41,361] EP:1 global_step:2130 loss:0.1054 perplexity:1.1111
[2022-12-28 22:30:43,458] EP:1 global_step:2140 loss:0.2130 perplexity:1.2374
[2022-12-28 22:30:45,598] EP:1 global_step:2150 loss:0.1522 perplexity:1.1644
[2022-12-28 22:30:47,743] EP:1 global_step:2160 loss:0.0875 perplexity:1.0914
[2022-12-28 22:30:49,885] EP:1 global_step:2170 loss:0.1259 perplexity:1.1342
[2022-12-28 22:30:52,098] EP:1 global_step:2180 loss:0.0999 perplexity:1.1050
[2022-12-28 22:30:54,197] EP:1 global_step:2190 loss:0.0544 perplexity:1.0559
[2022-12-28 22:30:56,355] EP:1 global_step:2200 loss:0.0835 perplexity:1.0871
[2022-12-28 22:31:10,629] [EVAL] global_step:2200 loss:0.3220 perplexity:1.3799
[2022-12-28 22:31:10,631] global_step: 2200 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_2200.pth
[2022-12-28 22:31:13,497] EP:1 global_step:2210 loss:0.0980 perplexity:1.1030
[2022-12-28 22:31:15,698] EP:1 global_step:2220 loss:0.1339 perplexity:1.1433
[2022-12-28 22:31:17,934] EP:1 global_step:2230 loss:0.1218 perplexity:1.1295
[2022-12-28 22:31:19,955] EP:1 global_step:2240 loss:0.1291 perplexity:1.1378
[2022-12-28 22:31:22,104] EP:1 global_step:2250 loss:0.0847 perplexity:1.0884
[2022-12-28 22:31:24,297] EP:1 global_step:2260 loss:0.1069 perplexity:1.1129
[2022-12-28 22:31:26,336] EP:1 global_step:2270 loss:0.1153 perplexity:1.1222
[2022-12-28 22:31:28,499] EP:1 global_step:2280 loss:0.1298 perplexity:1.1385
[2022-12-28 22:31:30,611] EP:1 global_step:2290 loss:0.1119 perplexity:1.1183
[2022-12-28 22:31:32,660] EP:1 global_step:2300 loss:0.1312 perplexity:1.1402
[2022-12-28 22:31:34,687] EP:1 global_step:2310 loss:0.1015 perplexity:1.1068
[2022-12-28 22:31:36,827] EP:1 global_step:2320 loss:0.1607 perplexity:1.1743
[2022-12-28 22:31:38,806] EP:1 global_step:2330 loss:0.0790 perplexity:1.0822
[2022-12-28 22:31:41,073] EP:1 global_step:2340 loss:0.0828 perplexity:1.0864
[2022-12-28 22:31:43,402] EP:2 global_step:2350 loss:0.1103 perplexity:1.1166
[2022-12-28 22:31:45,606] EP:2 global_step:2360 loss:0.0772 perplexity:1.0802
[2022-12-28 22:31:47,771] EP:2 global_step:2370 loss:0.0850 perplexity:1.0888
[2022-12-28 22:31:49,907] EP:2 global_step:2380 loss:0.0918 perplexity:1.0961
[2022-12-28 22:31:52,077] EP:2 global_step:2390 loss:0.0737 perplexity:1.0765
[2022-12-28 22:31:54,346] EP:2 global_step:2400 loss:0.0738 perplexity:1.0765
[2022-12-28 22:32:08,619] [EVAL] global_step:2400 loss:0.3131 perplexity:1.3677
[2022-12-28 22:32:08,622] global_step: 2400 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_2400.pth
[2022-12-28 22:32:11,564] EP:2 global_step:2410 loss:0.0493 perplexity:1.0505
[2022-12-28 22:32:13,761] EP:2 global_step:2420 loss:0.1744 perplexity:1.1906
[2022-12-28 22:32:15,818] EP:2 global_step:2430 loss:0.0978 perplexity:1.1028
[2022-12-28 22:32:17,939] EP:2 global_step:2440 loss:0.1054 perplexity:1.1112
[2022-12-28 22:32:20,072] EP:2 global_step:2450 loss:0.0862 perplexity:1.0901
[2022-12-28 22:32:22,235] EP:2 global_step:2460 loss:0.1010 perplexity:1.1063
[2022-12-28 22:32:24,400] EP:2 global_step:2470 loss:0.1117 perplexity:1.1182
[2022-12-28 22:32:26,610] EP:2 global_step:2480 loss:0.0752 perplexity:1.0781
[2022-12-28 22:32:28,777] EP:2 global_step:2490 loss:0.0821 perplexity:1.0856
[2022-12-28 22:32:30,930] EP:2 global_step:2500 loss:0.0670 perplexity:1.0693
[2022-12-28 22:32:32,989] EP:2 global_step:2510 loss:0.0380 perplexity:1.0387
[2022-12-28 22:32:35,091] EP:2 global_step:2520 loss:0.1316 perplexity:1.1407
[2022-12-28 22:32:37,256] EP:2 global_step:2530 loss:0.0690 perplexity:1.0714
[2022-12-28 22:32:39,344] EP:2 global_step:2540 loss:0.0526 perplexity:1.0540
[2022-12-28 22:32:41,462] EP:2 global_step:2550 loss:0.0623 perplexity:1.0643
[2022-12-28 22:32:43,603] EP:2 global_step:2560 loss:0.0530 perplexity:1.0544
[2022-12-28 22:32:45,702] EP:2 global_step:2570 loss:0.0936 perplexity:1.0981
[2022-12-28 22:32:47,837] EP:2 global_step:2580 loss:0.0573 perplexity:1.0590
[2022-12-28 22:32:50,059] EP:2 global_step:2590 loss:0.0808 perplexity:1.0841
[2022-12-28 22:32:52,155] EP:2 global_step:2600 loss:0.0205 perplexity:1.0207
[2022-12-28 22:33:06,425] [EVAL] global_step:2600 loss:0.3568 perplexity:1.4287
[2022-12-28 22:33:06,427] global_step: 2600 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_2600.pth
[2022-12-28 22:33:09,288] EP:2 global_step:2610 loss:0.0227 perplexity:1.0230
[2022-12-28 22:33:11,413] EP:2 global_step:2620 loss:0.0622 perplexity:1.0642
[2022-12-28 22:33:13,598] EP:2 global_step:2630 loss:0.0669 perplexity:1.0692
[2022-12-28 22:33:15,764] EP:2 global_step:2640 loss:0.0461 perplexity:1.0472
[2022-12-28 22:33:17,934] EP:2 global_step:2650 loss:0.0426 perplexity:1.0435
[2022-12-28 22:33:20,056] EP:2 global_step:2660 loss:0.0338 perplexity:1.0344
[2022-12-28 22:33:22,259] EP:2 global_step:2670 loss:0.0820 perplexity:1.0854
[2022-12-28 22:33:24,463] EP:2 global_step:2680 loss:0.0676 perplexity:1.0699
[2022-12-28 22:33:26,539] EP:2 global_step:2690 loss:0.0212 perplexity:1.0214
[2022-12-28 22:33:28,841] EP:2 global_step:2700 loss:0.0711 perplexity:1.0737
[2022-12-28 22:33:30,882] EP:2 global_step:2710 loss:0.0186 perplexity:1.0188
[2022-12-28 22:33:33,011] EP:2 global_step:2720 loss:0.0384 perplexity:1.0391
[2022-12-28 22:33:35,118] EP:2 global_step:2730 loss:0.0402 perplexity:1.0410
[2022-12-28 22:33:37,212] EP:2 global_step:2740 loss:0.0212 perplexity:1.0214
[2022-12-28 22:33:39,336] EP:2 global_step:2750 loss:0.0587 perplexity:1.0605
[2022-12-28 22:33:41,520] EP:2 global_step:2760 loss:0.0523 perplexity:1.0537
[2022-12-28 22:33:43,611] EP:2 global_step:2770 loss:0.0582 perplexity:1.0600
[2022-12-28 22:33:45,826] EP:2 global_step:2780 loss:0.0600 perplexity:1.0618
[2022-12-28 22:33:48,084] EP:2 global_step:2790 loss:0.0338 perplexity:1.0344
[2022-12-28 22:33:50,186] EP:2 global_step:2800 loss:0.0244 perplexity:1.0247
[2022-12-28 22:34:04,454] [EVAL] global_step:2800 loss:0.3891 perplexity:1.4756
[2022-12-28 22:34:04,457] global_step: 2800 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_2800.pth
[2022-12-28 22:34:07,369] EP:2 global_step:2810 loss:0.0394 perplexity:1.0401
[2022-12-28 22:34:09,527] EP:2 global_step:2820 loss:0.0183 perplexity:1.0185
[2022-12-28 22:34:11,663] EP:2 global_step:2830 loss:0.0445 perplexity:1.0455
[2022-12-28 22:34:13,747] EP:2 global_step:2840 loss:0.0695 perplexity:1.0720
[2022-12-28 22:34:15,918] EP:2 global_step:2850 loss:0.0372 perplexity:1.0379
[2022-12-28 22:34:18,154] EP:2 global_step:2860 loss:0.0439 perplexity:1.0449
[2022-12-28 22:34:20,305] EP:2 global_step:2870 loss:0.0397 perplexity:1.0405
[2022-12-28 22:34:22,491] EP:2 global_step:2880 loss:0.0576 perplexity:1.0593
[2022-12-28 22:34:24,741] EP:2 global_step:2890 loss:0.0370 perplexity:1.0377
[2022-12-28 22:34:26,890] EP:2 global_step:2900 loss:0.0624 perplexity:1.0644
[2022-12-28 22:34:28,926] EP:2 global_step:2910 loss:0.0219 perplexity:1.0221
[2022-12-28 22:34:31,103] EP:2 global_step:2920 loss:0.0330 perplexity:1.0336
[2022-12-28 22:34:33,220] EP:2 global_step:2930 loss:0.0868 perplexity:1.0907
[2022-12-28 22:34:35,375] EP:2 global_step:2940 loss:0.0386 perplexity:1.0394
[2022-12-28 22:34:37,515] EP:2 global_step:2950 loss:0.0823 perplexity:1.0858
[2022-12-28 22:34:39,691] EP:2 global_step:2960 loss:0.0196 perplexity:1.0198
[2022-12-28 22:34:41,850] EP:2 global_step:2970 loss:0.0314 perplexity:1.0318
[2022-12-28 22:34:43,963] EP:2 global_step:2980 loss:0.0442 perplexity:1.0452
[2022-12-28 22:34:46,014] EP:2 global_step:2990 loss:0.0454 perplexity:1.0464
[2022-12-28 22:34:48,120] EP:2 global_step:3000 loss:0.0476 perplexity:1.0488
[2022-12-28 22:35:02,367] [EVAL] global_step:3000 loss:0.3920 perplexity:1.4799
[2022-12-28 22:35:02,369] global_step: 3000 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_3000.pth
[2022-12-28 22:35:05,234] EP:2 global_step:3010 loss:0.0719 perplexity:1.0746
[2022-12-28 22:35:07,384] EP:2 global_step:3020 loss:0.0332 perplexity:1.0338
[2022-12-28 22:35:09,626] EP:2 global_step:3030 loss:0.0441 perplexity:1.0451
[2022-12-28 22:35:11,804] EP:2 global_step:3040 loss:0.0737 perplexity:1.0765
[2022-12-28 22:35:13,962] EP:2 global_step:3050 loss:0.0430 perplexity:1.0439
[2022-12-28 22:35:16,135] EP:2 global_step:3060 loss:0.0228 perplexity:1.0231
[2022-12-28 22:35:18,328] EP:2 global_step:3070 loss:0.0358 perplexity:1.0365
[2022-12-28 22:35:20,447] EP:2 global_step:3080 loss:0.0622 perplexity:1.0642
[2022-12-28 22:35:22,623] EP:2 global_step:3090 loss:0.0388 perplexity:1.0396
[2022-12-28 22:35:24,870] EP:2 global_step:3100 loss:0.0641 perplexity:1.0662
[2022-12-28 22:35:27,084] EP:2 global_step:3110 loss:0.0586 perplexity:1.0604
[2022-12-28 22:35:29,306] EP:2 global_step:3120 loss:0.0596 perplexity:1.0614
[2022-12-28 22:35:31,505] EP:2 global_step:3130 loss:0.0415 perplexity:1.0423
[2022-12-28 22:35:33,625] EP:2 global_step:3140 loss:0.0941 perplexity:1.0987
[2022-12-28 22:35:35,728] EP:2 global_step:3150 loss:0.0318 perplexity:1.0323
[2022-12-28 22:35:37,978] EP:2 global_step:3160 loss:0.0307 perplexity:1.0312
[2022-12-28 22:35:40,050] EP:2 global_step:3170 loss:0.0387 perplexity:1.0395
[2022-12-28 22:35:42,251] EP:2 global_step:3180 loss:0.0201 perplexity:1.0203
[2022-12-28 22:35:44,431] EP:2 global_step:3190 loss:0.0175 perplexity:1.0177
[2022-12-28 22:35:46,677] EP:2 global_step:3200 loss:0.0138 perplexity:1.0139
[2022-12-28 22:36:00,949] [EVAL] global_step:3200 loss:0.4025 perplexity:1.4955
[2022-12-28 22:36:00,951] global_step: 3200 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_3200.pth
[2022-12-28 22:36:03,881] EP:2 global_step:3210 loss:0.0645 perplexity:1.0666
[2022-12-28 22:36:05,957] EP:2 global_step:3220 loss:0.0340 perplexity:1.0346
[2022-12-28 22:36:08,042] EP:2 global_step:3230 loss:0.0237 perplexity:1.0240
[2022-12-28 22:36:10,330] EP:2 global_step:3240 loss:0.0503 perplexity:1.0516
[2022-12-28 22:36:12,462] EP:2 global_step:3250 loss:0.0245 perplexity:1.0248
[2022-12-28 22:36:14,531] EP:2 global_step:3260 loss:0.0097 perplexity:1.0098
[2022-12-28 22:36:16,747] EP:2 global_step:3270 loss:0.0324 perplexity:1.0329
[2022-12-28 22:36:18,897] EP:2 global_step:3280 loss:0.0284 perplexity:1.0288
[2022-12-28 22:36:21,099] EP:2 global_step:3290 loss:0.0479 perplexity:1.0490
[2022-12-28 22:36:23,351] EP:2 global_step:3300 loss:0.0494 perplexity:1.0507
[2022-12-28 22:36:25,374] EP:2 global_step:3310 loss:0.1058 perplexity:1.1116
[2022-12-28 22:36:27,534] EP:2 global_step:3320 loss:0.0377 perplexity:1.0384
[2022-12-28 22:36:29,663] EP:2 global_step:3330 loss:0.0526 perplexity:1.0540
[2022-12-28 22:36:31,812] EP:2 global_step:3340 loss:0.0242 perplexity:1.0245
[2022-12-28 22:36:33,957] EP:2 global_step:3350 loss:0.0375 perplexity:1.0382
[2022-12-28 22:36:36,142] EP:2 global_step:3360 loss:0.0342 perplexity:1.0348
[2022-12-28 22:36:38,335] EP:2 global_step:3370 loss:0.0150 perplexity:1.0151
[2022-12-28 22:36:40,487] EP:2 global_step:3380 loss:0.0085 perplexity:1.0086
[2022-12-28 22:36:42,678] EP:2 global_step:3390 loss:0.0577 perplexity:1.0594
[2022-12-28 22:36:44,889] EP:2 global_step:3400 loss:0.0536 perplexity:1.0551
[2022-12-28 22:36:59,132] [EVAL] global_step:3400 loss:0.4070 perplexity:1.5022
[2022-12-28 22:36:59,134] global_step: 3400 model saved at kc_bert/bert_2022.12.28_22.19.17/gpt2_step_3400.pth
[2022-12-28 22:37:01,908] EP:2 global_step:3410 loss:0.0460 perplexity:1.0470
[2022-12-28 22:37:04,041] EP:2 global_step:3420 loss:0.0519 perplexity:1.0533
[2022-12-28 22:37:06,189] EP:2 global_step:3430 loss:0.0660 perplexity:1.0682
[2022-12-28 22:37:08,273] EP:2 global_step:3440 loss:0.0479 perplexity:1.0490
[2022-12-28 22:37:10,441] EP:2 global_step:3450 loss:0.0372 perplexity:1.0379
[2022-12-28 22:37:12,527] EP:2 global_step:3460 loss:0.0525 perplexity:1.0539
[2022-12-28 22:37:14,588] EP:2 global_step:3470 loss:0.0439 perplexity:1.0449
[2022-12-28 22:37:16,625] EP:2 global_step:3480 loss:0.0299 perplexity:1.0304
[2022-12-28 22:37:18,755] EP:2 global_step:3490 loss:0.0583 perplexity:1.0600
[2022-12-28 22:37:20,773] EP:2 global_step:3500 loss:0.0230 perplexity:1.0232
[2022-12-28 22:37:22,989] EP:2 global_step:3510 loss:0.0525 perplexity:1.0539
